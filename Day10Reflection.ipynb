{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day10Reflection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuTdgakltYmy"
      },
      "source": [
        "1.How do you think Machine Learning or AI concepts were utilized in the design of this game?\n",
        "\n",
        "This is a very good example of unsupervised learning machine learning AI game becuase the dataset's decision were heavely effected by finding the patterns within previous human made decisions. Now it was intentional that the dataset was havely orange prior to the data set being given to the computer to ensure that the bias was there, even if unintentional. This shift from heavly blue to a more mixed set of data and pre-existing, unintentional bias caused the company to lose an amazing  possible employee. Emily, was a perfect canidate for the job, but the computer let her go, while letting in more less experienced or educated or people with less ambitions purly because they were orange. Now I have learned that The sampling process of the data collection must be free of biases or else it will prevent people from getting the job or the healthcare purely for incorrect training. One possible way might be to remove such attributes when working with data that could cause unecessay patterns.\n",
        "\n",
        "\n",
        "2.Can you give a real-world example of a biased machine learning model, and share your ideas on how you make this model more fair, inclusive, and equitable? Please reflect on why you selected this specific biased model.\n",
        "\n",
        "https://www.technologyreview.com/2019/01/21/137783/algorithms-criminal-justice-ai\n",
        "\n",
        "POC make 13 precent of the population, while making 50 precent of the prison population. On average, they are imporsioned and lation men for much longer times then any other race or possible gender. These decisions are based of the many sterotypes associated with the groups. This means that there is already a horrible track record, so the data is already biased. Introducing that data to an AI to choose whether or not someone is more likely to commit a crime or how long they should stay in prison is going to be hurt by the preset biases in the data. On top of which, due to it being AI and so new, many prisoners struggle to appeal the decision of the court. How can that be, they are hurt by preset biases that they may not even know are there or understand how it works and have to struggle to appeal that decisions purely due to limited human knowledge. That is both unfair, unethical, and more likely unconstitutional. THERE NEEDS TO BE A CHANGE\n"
      ]
    }
  ]
}