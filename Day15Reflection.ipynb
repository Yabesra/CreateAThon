{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day15Reflection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDfexhZEgx4N"
      },
      "source": [
        "Write a reflection piece on the advantages of the Rectified Linear activation function, along with one use case. Share your reflection in the Responses section of your README.\n",
        "\n",
        "The rectified linear activation function or ReLU is an activation function that helps Nueral Networks when tresnforming the summed weighted inout between nodes or between output and input.The ReLU is a peicewise function that will wokr when poisitves, otherwise it gives us a zero. Thos means we can easily backpropagate errors and train multiple layers of nuerons. On top of which it is much simpler than sigmoid and takes less energy for computers to do. A zero output lets for little amount of neurons to work making the network sparse and efficitent. Not to mention, ReLU learns much faster than Sigmoid and Tanh. the only issue is the higher the learning rate, the higher chance of neurons dying(menaing they only put out 0s at that point). This means its best to use a v ariotion of ReLU. \n",
        "\n",
        "A one time use of ReLU might be when you have one hidden function or when using it for simply math equations. "
      ]
    }
  ]
}